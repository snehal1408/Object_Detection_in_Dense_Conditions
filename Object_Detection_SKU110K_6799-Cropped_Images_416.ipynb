{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Object_Detection_SKU110K_6799-Cropped_Images_416.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5aQ2GXmPKmD3"},"source":["# **This colab file shows how to detect object using YOLOv5 and SKU110K-6799_cropped_images dataset (which is made after cropping images from subset of SKU110K dataset (SKU110k_6799))**\n","\n","Code Reference links: \n","\n","1. https://github.com/ultralytics/yolov5\n","\n","2. https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb#scrollTo=7mGmQbAO5pQb\n","\n","3. https://www.youtube.com/watch?v=2nR2e4J4ZaI&t=1025s\n","\n","4. https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data"]},{"cell_type":"markdown","metadata":{"id":"FFh6ZUypqoxN"},"source":["# To Setup\n","\n","Clone repo, install dependencies and check PyTorch and GPU."]},{"cell_type":"code","metadata":{"id":"7iQJe5hU63J_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631814713618,"user_tz":-60,"elapsed":9238,"user":{"displayName":"snehal desai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisnRT1BHHCHN6bQBg49fvUCbrGKT_G74JRwbnl=s64","userId":"08420235973951776316"}},"outputId":"1ebfedf9-a458-43a7-e688-249165572cc0"},"source":["!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd yolov5\n","%pip install -qr requirements.txt  # install dependencies\n","\n","import torch\n","from IPython.display import Image, clear_output  # to display images\n","\n","clear_output()\n","print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Setup complete. Using torch 1.9.0+cu102 (CPU)\n"]}]},{"cell_type":"markdown","metadata":{"id":"y82ZlC8iMmff"},"source":["# To Use the Dataset \n","\n","* ### There are three ways to use the dataset:\n","\n","1. Download dataset in your local PC and upload dataset directly on colab notebook by clicking on `Files icon in left panel-->right click and upload--->choose dataset path-->click open`. So it will be uploaded in colab notebook. But keep in mind when notebook is connected, you need to upload every time again.\n","\n","2. Download dataset using `!wget` command and unzip it using `!unzip` command. In this option, you need to run the command every time once notebook is disconnected, so it will take long time to download and unzip the dataset, it depends on datset size.\n","\n","3. The best way to use the large datset is to save the dataset in your Google drive and use directly after mounting google drive."]},{"cell_type":"code","metadata":{"id":"6GkDr6nlr24p"},"source":["# Method 1 is directly upload in colab as explained in above point 1\n","# Method 2\n","# To download and unzip the SKU110K-6799_cropped_images dataset\n","\n","#!wget {URL link to download dataset} # To download the dataset\n","#!tar -xvf {path to untar the dataset} #To untar the dataset\n","#!unzip {path to unzip the dataset} # to unzip the datset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DStflr0DMm68","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631814746429,"user_tz":-60,"elapsed":25369,"user":{"displayName":"snehal desai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisnRT1BHHCHN6bQBg49fvUCbrGKT_G74JRwbnl=s64","userId":"08420235973951776316"}},"outputId":"a6b5ac51-b621-4c0f-b451-4d71a25597fb"},"source":["# Method 3\n","# To mount Google drive to use SKU110K-6799_cropped_images dataset\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}]},{"cell_type":"code","metadata":{"id":"1UXLr7BM4RDD"},"source":["#!python /content/prepare_images.py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0MbZrETqKrhM"},"source":["# For 50 Epochs and 16 Batch Size\n","\n","## 1. Train \n","\n","### Train command represents following parameters:\n","\n",">*   **`--img`**: Define input image size. The original image size is 1024 Ã— 1024, compress to smaller size make the training process faster.\n",">*   **`--batch`**: Determine the batch size. For example, if the batch size is 16 and the training set contains 8219 images, so the number of batches will be 8219 Ã· 16 = 513. \n",">*   **`--epochs`**: Define the number of training epochs. An epoch is responsible for learning all input images, in other words, training all input.  The number of epochs represents the number of times the model trains all the inputs and updates the weights to get closer to the ground truth labels.\n",">*   **`--data`**: The path to data.yaml file containing the summary of the dataset. The model evaluation process is executed immediately after each epoch, so the model will also access the validation directory via the path in data.yaml file and use its contents for \n","evaluation at that moment.\n",">*   **`--weights`**: Specify a path to weights. A pretrained weight can be used for saving training time. If it is left blank or not given, the model will automatically initialize random weights for training.\n",">*   **`--project`**: Name of result folder. The model will store all the results performed during training in a directory. If it is not given, than all training results are saved to runs/train/ with incrementing run directories, i.e. runs/train/exp2, runs/train/exp3 etc.\n",">*   **`--cache`**: Cache images for training faster.\n","\n","### This part shows training using YOLOv5 with Custom Dataset (SKU110K-6799_cropped_images) with 50 epochs, 416 image size and 16 batch size.\n","\n","1. In the {x}.yaml file, give all the path of train, test and validation folders from the dataset.\n","2. Give {x}.yaml file path as `--data` argumant in following command.\n","3. Give your results folder's path as `--project` argument, where you want to save your results."]},{"cell_type":"code","metadata":{"id":"qfOaNjkC7UNK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b8e4f3bc-02cc-4a1c-a16b-b879d433101b"},"source":["# To train using YOLOv5 and dataset SKU110K-6799_cropped_images with 50 epochs, 416 image size and 16 batch size\n","\n","!python train.py --img 416 --batch 16 --epochs 50 --data /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799-Cropped_images.yaml --project /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU100K-6799-Cropped_images_results/16_416_50 "],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799-Cropped_images.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=50, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache_images=False, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU100K-6799-Cropped_images_results/16_416_50, entity=None, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n","YOLOv5 ðŸš€ v5.0-315-g96e36a7 torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU100K-6799-Cropped_images_results/16_416_50', view at http://localhost:6006/\n","2021-07-25 19:54:29.500263: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt to yolov5s.pt...\n","100% 14.1M/14.1M [00:00<00:00, 55.2MB/s]\n","\n","Overriding model.yaml nc=80 with nc=1\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n","  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","Model Summary: 283 layers, 7063542 parameters, 7063542 gradients, 16.4 GFLOPs\n","\n","Transferred 356/362 items from yolov5s.pt\n","Scaled weight_decay = 0.0005\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 59 weight, 62 weight (no decay), 62 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799_Cropped_images/training/labels.cache' images and labels... 46293 found, 6920 missing, 702 empty, 0 corrupted: 100% 53213/53213 [00:00<00:00, 516646987.85it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799_Cropped_images/validation/labels.cache' images and labels... 4460 found, 0 missing, 59 empty, 0 corrupted: 100% 4460/4460 [00:00<00:00, 32703838.88it/s]\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Plotting labels... \n","\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 5.06, Best Possible Recall (BPR) = 0.9711. Attempting to improve anchors, please wait...\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mWARNING: Extremely small objects found. 23465 of 1123542 labels are < 3 pixels in size.\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 9 anchors on 1123489 points...\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9597 best possible recall, 6.88 anchors past thr\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=416, metric_all=0.413/0.729-mean/best, past_thr=0.490-mean: 27,25,  56,24,  24,66,  50,58,  36,99,  88,42,  44,145,  151,42,  77,102\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7420: 100% 1000/1000 [05:07<00:00,  3.25it/s]\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9687 best possible recall, 6.97 anchors past thr\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=416, metric_all=0.423/0.746-mean/best, past_thr=0.499-mean: 31,25,  61,19,  18,66,  41,44,  34,78,  69,50,  124,32,  40,116,  61,95\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mOriginal anchors better than new anchors. Proceeding with original anchors.\n","\n","Image sizes 416 train, 416 val\n","Using 2 dataloader workers\n","Logging results to /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU100K-6799-Cropped_images_results/16_416_50/exp\n","Starting training for 50 epochs...\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","      0/49      1.5G   0.05822     0.137         0       377       416: 100% 3326/3326 [2:47:04<00:00,  3.01s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [11:20<00:00,  4.86s/it]\n","                 all       4460     113340      0.885      0.771      0.849      0.494\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","      1/49      1.7G   0.04301    0.1321         0       350       416: 100% 3326/3326 [1:00:00<00:00,  1.08s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:48<00:00,  1.29it/s]\n","                 all       4460     113340      0.902      0.787      0.868      0.523\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","      2/49      1.7G   0.04151    0.1309         0       374       416: 100% 3326/3326 [1:00:06<00:00,  1.08s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:47<00:00,  1.31it/s]\n","                 all       4460     113340      0.902      0.799      0.873      0.529\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","      3/49      1.7G   0.04051    0.1302         0       412       416: 100% 3326/3326 [1:00:13<00:00,  1.09s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:48<00:00,  1.29it/s]\n","                 all       4460     113340      0.905       0.81      0.879      0.538\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","      4/49      1.7G   0.03946    0.1286         0       331       416: 100% 3326/3326 [1:00:00<00:00,  1.08s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:48<00:00,  1.29it/s]\n","                 all       4460     113340      0.908      0.816      0.885      0.547\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","      5/49      1.7G   0.03903    0.1272         0       298       416: 100% 3326/3326 [1:00:50<00:00,  1.10s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:49<00:00,  1.28it/s]\n","                 all       4460     113340      0.915      0.819      0.889      0.554\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","      6/49      1.7G   0.03853    0.1264         0       433       416: 100% 3326/3326 [1:00:51<00:00,  1.10s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:49<00:00,  1.28it/s]\n","                 all       4460     113340      0.915      0.817      0.889      0.556\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","      7/49      1.7G   0.03831     0.126         0       361       416: 100% 3326/3326 [1:00:36<00:00,  1.09s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:49<00:00,  1.27it/s]\n","                 all       4460     113340      0.915      0.826      0.893      0.561\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","      8/49      1.7G   0.03805    0.1255         0       324       416: 100% 3326/3326 [1:00:45<00:00,  1.10s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:50<00:00,  1.27it/s]\n","                 all       4460     113340      0.916      0.828      0.894      0.562\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","      9/49      1.7G   0.03795    0.1253         0       349       416: 100% 3326/3326 [1:01:17<00:00,  1.11s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:49<00:00,  1.28it/s]\n","                 all       4460     113340      0.915      0.831      0.895      0.564\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     10/49      1.7G   0.03773    0.1247         0       360       416: 100% 3326/3326 [1:01:11<00:00,  1.10s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:49<00:00,  1.28it/s]\n","                 all       4460     113340      0.914      0.833      0.895      0.564\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     11/49      1.7G   0.03763    0.1248         0       412       416: 100% 3326/3326 [1:01:09<00:00,  1.10s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:49<00:00,  1.28it/s]\n","                 all       4460     113340      0.919      0.832      0.898      0.567\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     12/49      1.7G    0.0375    0.1242         0       246       416: 100% 3326/3326 [1:00:54<00:00,  1.10s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:48<00:00,  1.29it/s]\n","                 all       4460     113340      0.916      0.836      0.899      0.568\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     13/49      1.7G   0.03743    0.1239         0       301       416: 100% 3326/3326 [1:00:58<00:00,  1.10s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:48<00:00,  1.29it/s]\n","                 all       4460     113340      0.914      0.838      0.899      0.569\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     14/49      1.7G   0.03729    0.1238         0       461       416: 100% 3326/3326 [1:00:32<00:00,  1.09s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:47<00:00,  1.30it/s]\n","                 all       4460     113340      0.917      0.836      0.899       0.57\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     15/49      1.7G   0.03717    0.1234         0       435       416: 100% 3326/3326 [1:00:38<00:00,  1.09s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:49<00:00,  1.28it/s]\n","                 all       4460     113340      0.918      0.837        0.9      0.571\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     16/49      1.7G   0.03704    0.1231         0       362       416: 100% 3326/3326 [1:01:02<00:00,  1.10s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:47<00:00,  1.30it/s]\n","                 all       4460     113340       0.92      0.836        0.9      0.571\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     17/49      1.7G     0.037    0.1231         0       374       416: 100% 3326/3326 [1:01:28<00:00,  1.11s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:47<00:00,  1.30it/s]\n","                 all       4460     113340      0.916       0.84        0.9      0.571\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     18/49      1.7G   0.03695    0.1234         0       397       416: 100% 3326/3326 [1:00:42<00:00,  1.10s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:46<00:00,  1.31it/s]\n","                 all       4460     113340      0.919      0.838        0.9      0.572\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     19/49      1.7G   0.03686    0.1234         0       317       416: 100% 3326/3326 [1:00:55<00:00,  1.10s/it]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 140/140 [01:48<00:00,  1.29it/s]\n","                 all       4460     113340      0.918      0.838      0.901      0.572\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","     20/49      1.7G   0.03683     0.123         0       328       416:  96% 3180/3326 [58:22<02:51,  1.17s/it]"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rjNFCAQYK2Qy"},"source":["## 2. Validate\n","\n","* Use above trained model `best.pt` as `--weights` argument and validate a model's accuracy with 416 image size on SKU110K-6799_good_images dataset using following command line.\n","\n","* All validation results are saved to runs/val/ with incrementing run directories, i.e. runs/val/exp2, runs/val/exp3 etc., if `--project` argument is not given."]},{"cell_type":"code","metadata":{"id":"1ncAIC1NC-ZF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631815659026,"user_tz":-60,"elapsed":269764,"user":{"displayName":"snehal desai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisnRT1BHHCHN6bQBg49fvUCbrGKT_G74JRwbnl=s64","userId":"08420235973951776316"}},"outputId":"984e5374-6479-401d-cb0a-5cb21f1b7756"},"source":["# To validate model's accuracy using 416 image size on SKU110K-6799_cropped_images\n","\n","!python val.py --task test --batch-size 16 --img 416 --weights /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799_16_416_50_crops_best.pt --data /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799-Cropped_images.yaml --project /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU100K-6799-Cropped_images_results/16_416_50"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799-Cropped_images.yaml, weights=['/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU100K-6799-Cropped_images_results/16_416_50/train/weights/best.pt'], batch_size=16, imgsz=416, conf_thres=0.001, iou_thres=0.6, task=test, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU100K-6799-Cropped_images_results/16_416_50, name=exp, exist_ok=False, half=False\n","YOLOv5 ðŸš€ v5.0-438-g27a4736 torch 1.9.0+cu102 CPU\n","\n","Fusing layers... \n","Model Summary: 224 layers, 7053910 parameters, 0 gradients, 16.3 GFLOPs\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning '/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799_Cropped_images/evaluation/labels.cache' images and labels... 0 found, 1186 missing, 0 empty, 1 corrupted: 100% 1187/1187 [00:00<?, ?it/s]\u001b[34m\u001b[1mtest: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799_Cropped_images/evaluation/images/test_600_18.jpg: corrupted JPEG\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning '/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799_Cropped_images/evaluation/labels.cache' images and labels... 0 found, 1186 missing, 0 empty, 1 corrupted: 100% 1187/1187 [00:00<?, ?it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 75/75 [04:23<00:00,  3.51s/it]\n","                 all       1186          0          0          0          0          0\n","Speed: 1.0ms pre-process, 176.3ms inference, 2.1ms NMS per image at shape (16, 3, 416, 416)\n","Results saved to \u001b[1m/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU100K-6799-Cropped_images_results/16_416_50/exp\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"g_RjKGmaLFaJ"},"source":["## 3. Detect\n","\n","* Use following command to detect the objects from images. You can set your detection images path using `--source` argument. \n","\n","* All detecting results are saved to runs/detect/ with incrementing run directories, i.e. runs/detect/exp2, runs/detect/exp3 etc., if `--project` argument is not given."]},{"cell_type":"code","metadata":{"id":"HVCp_nTSC13z"},"source":["# To detect objects from given source path's images\n","# To detect test_0.jpg\n","\n","!python detect.py --img 416 --weights /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799_16_416_50_crops_best.pt --data /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799-Cropped_images.yaml --project /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU100K-6799-Cropped_images_results/16_416_50  --source /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU-110K/images/test_0.jpg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pFWrp7y3LerP"},"source":["# To Convert Folder into Zip Folder"]},{"cell_type":"code","metadata":{"id":"j0mW_pZDLdog"},"source":["import os\n","from os import path\n","import shutil\n","from shutil import make_archive\n","from zipfile import ZipFile\n","!zip -r /content/results.zip /content/yolov5/runs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OMFDiLgxLsJH"},"source":["# To Download Zipped Folder from Colab to Local PC"]},{"cell_type":"code","metadata":{"id":"O9aELHdYLsgc"},"source":["from google.colab import files\n","files.download('/content/results.zip')"],"execution_count":null,"outputs":[]}]}