{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Object_Detection_SKU110K_6799-Cropped_Images_640.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5aQ2GXmPKmD3"},"source":["# **This colab file shows how to detect object using YOLOv5 and SKU110K-6799_cropped_images dataset (which is made after cropping images from subset of SKU110K dataset (SKU110k_6799))**\n","\n","Code Reference links: \n","\n","1. https://github.com/ultralytics/yolov5\n","\n","2. https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb#scrollTo=7mGmQbAO5pQb\n","\n","3. https://www.youtube.com/watch?v=2nR2e4J4ZaI&t=1025s\n","\n","4. https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data"]},{"cell_type":"markdown","metadata":{"id":"6MkKTWn4kGp3"},"source":["# To Setup\n","\n","Clone repo, install dependencies and check PyTorch and GPU."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7iQJe5hU63J_","executionInfo":{"status":"ok","timestamp":1631815486005,"user_tz":-60,"elapsed":10426,"user":{"displayName":"snehal desai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisnRT1BHHCHN6bQBg49fvUCbrGKT_G74JRwbnl=s64","userId":"08420235973951776316"}},"outputId":"05943e91-68a5-4850-d98a-35e171a156fe"},"source":["!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd yolov5\n","%pip install -qr requirements.txt  # install dependencies\n","\n","import torch\n","from IPython.display import Image, clear_output  # to display images\n","\n","clear_output()\n","print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Setup complete. Using torch 1.9.0+cu102 (Tesla T4)\n"]}]},{"cell_type":"markdown","metadata":{"id":"y82ZlC8iMmff"},"source":["# To Use the Dataset \n","\n","* ### There are three ways to use the dataset:\n","\n","1. Download dataset in your local PC and upload dataset directly on colab notebook by clicking on `Files icon in left panel-->right click and upload--->choose dataset path-->click open`. So it will be uploaded in colab notebook. But keep in mind when notebook is connected, you need to upload every time again.\n","\n","2. Download dataset using `!wget` command and unzip it using `!unzip` command. In this option, you need to run the command every time once notebook is disconnected, so it will take long time to download and unzip the dataset, it depends on datset size.\n","\n","3. The best way to use the large datset is to save the dataset in your Google drive and use directly after mounting google drive."]},{"cell_type":"code","metadata":{"id":"-_45cHqskZ0X"},"source":["# Method 1 is directly upload in colab as explained in above point 1\n","# Method 2\n","# To download and unzip the SKU110K-6799_cropped_images dataset\n","\n","#!wget {URL link to download dataset} # To download the dataset\n","#!tar -xvf {path to untar the dataset} #To untar the dataset\n","#!unzip {path to unzip the dataset} # to unzip the datset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DStflr0DMm68","executionInfo":{"status":"ok","timestamp":1631815517494,"user_tz":-60,"elapsed":24372,"user":{"displayName":"snehal desai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisnRT1BHHCHN6bQBg49fvUCbrGKT_G74JRwbnl=s64","userId":"08420235973951776316"}},"outputId":"f90e61ee-da5a-4ca6-fbdf-d2b3cff8c77d"},"source":["# Method 3\n","# To mount Google drive to use SKU110K-6799_cropped_images dataset\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}]},{"cell_type":"code","metadata":{"id":"1UXLr7BM4RDD"},"source":["#!python /content/prepare_images.py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0MbZrETqKrhM"},"source":["# For 50 Epochs and 16 Batch Size\n","\n","## 1. Train \n","\n","### Train command represents following parameters:\n","\n",">*   **`--img`**: Define input image size. The original image size is 1024 Ã— 1024, compress to smaller size make the training process faster.\n",">*   **`--batch`**: Determine the batch size. For example, if the batch size is 16 and the training set contains 8219 images, so the number of batches will be 8219 Ã· 16 = 513. \n",">*   **`--epochs`**: Define the number of training epochs. An epoch is responsible for learning all input images, in other words, training all input.  The number of epochs represents the number of times the model trains all the inputs and updates the weights to get closer to the ground truth labels.\n",">*   **`--data`**: The path to data.yaml file containing the summary of the dataset. The model evaluation process is executed immediately after each epoch, so the model will also access the validation directory via the path in data.yaml file and use its contents for \n","evaluation at that moment.\n",">*   **`--weights`**: Specify a path to weights. A pretrained weight can be used for saving training time. If it is left blank or not given, the model will automatically initialize random weights for training.\n",">*   **`--project`**: Name of result folder. The model will store all the results performed during training in a directory. If it is not given, than all training results are saved to runs/train/ with incrementing run directories, i.e. runs/train/exp2, runs/train/exp3 etc.\n",">*   **`--cache`**: Cache images for training faster.\n","\n","### This part shows training using YOLOv5 with Custom Dataset (SKU110K-6799_cropped_images) with 50 epochs, 640 image size and 16 batch size.\n","\n","1. In the {x}.yaml file, give all the path of train, test and validation folders from the dataset.\n","2. Give {x}.yaml file path as `--data` argumant in following command.\n","3. Give your results folder's path as `--project` argument, where you want to save your results."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qfOaNjkC7UNK","executionInfo":{"elapsed":2239422,"status":"ok","timestamp":1626968310510,"user":{"displayName":"snehal desai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisnRT1BHHCHN6bQBg49fvUCbrGKT_G74JRwbnl=s64","userId":"08420235973951776316"},"user_tz":-60},"outputId":"948139d7-600e-466c-aa9f-2a62b6b3da1c"},"source":["# To train using YOLOv5 and dataset SKU110K-6799_cropped_images with 50 epochs, 640 image size and 16 batch size\n","\n","!python train.py --img 640 --batch 16 --epochs 50 --data /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799-Cropped_images.yaml --project /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU100K-6799-Cropped_images_results/16_640_50 --cache"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799-Cropped_images.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache_images=True, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU100K-6799-Cropped_images_results/640_50, entity=None, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n","YOLOv5 ðŸš€ v5.0-304-g2c073cd torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU100K-6799-Cropped_images_results/640_50', view at http://localhost:6006/\n","2021-07-22 15:01:14.555910: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOv5 logging with 'pip install wandb' (recommended)\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt to yolov5s.pt...\n","100% 14.1M/14.1M [00:00<00:00, 39.3MB/s]\n","\n","Overriding model.yaml nc=80 with nc=1\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n","  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","Model Summary: 283 layers, 7063542 parameters, 7063542 gradients, 16.4 GFLOPs\n","\n","Transferred 356/362 items from yolov5s.pt\n","Scaled weight_decay = 0.0005\n","Optimizer groups: 62 .bias, 62 conv.weight, 59 other\n","\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799_Cropped_images/training/labels.cache' images and labels... 46293 found, 6920 missing, 702 empty, 0 corrupted: 100% 53213/53213 [00:00<00:00, 635873215.82it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (23.1GB):  35% 18791/53213 [35:46<19:00:28,  1.99s/it]^C\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rjNFCAQYK2Qy"},"source":["## 2. Validate\n","\n","* Use above trained model `best.pt` as `--weights` argument and validate a model's accuracy with 640 image size on SKU110K-6799_good_images dataset using following command line.\n","\n","* All validation results are saved to runs/val/ with incrementing run directories, i.e. runs/val/exp2, runs/val/exp3 etc., if `--project` argument is not given."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ncAIC1NC-ZF","executionInfo":{"status":"ok","timestamp":1631816503899,"user_tz":-60,"elapsed":726767,"user":{"displayName":"snehal desai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisnRT1BHHCHN6bQBg49fvUCbrGKT_G74JRwbnl=s64","userId":"08420235973951776316"}},"outputId":"49d79cb4-2bd2-4572-fa81-dc0f982b0113"},"source":["# To validate model's accuracy using 640 image size on SKU110K-6799_cropped_images\n","\n","!python val.py --task test --batch-size 16 --img 640 --weights /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799_16_640_50_crops_best.pt --data /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799-Cropped_images.yaml --project /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU100K-6799-Cropped_images_results/16_640_50"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","100% 755k/755k [00:00<00:00, 29.2MB/s]\n","\u001b[34m\u001b[1mval: \u001b[0mdata=/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799-Cropped_images.yaml, weights=['/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799_16_640_50_crops_best.pt'], batch_size=16, imgsz=640, conf_thres=0.001, iou_thres=0.6, task=test, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU100K-6799-Cropped_images_results/16_640_50, name=exp, exist_ok=False, half=False\n","YOLOv5 ðŸš€ v5.0-438-g27a4736 torch 1.9.0+cu102 CUDA:0 (Tesla T4, 15109.75MB)\n","\n","Fusing layers... \n","Model Summary: 224 layers, 7053910 parameters, 0 gradients, 16.3 GFLOPs\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning '/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799_Cropped_images/evaluation/labels.cache' images and labels... 0 found, 1186 missing, 0 empty, 1 corrupted: 100% 1187/1187 [00:00<?, ?it/s]\u001b[34m\u001b[1mtest: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799_Cropped_images/evaluation/images/test_600_18.jpg: corrupted JPEG\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning '/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799_Cropped_images/evaluation/labels.cache' images and labels... 0 found, 1186 missing, 0 empty, 1 corrupted: 100% 1187/1187 [00:00<?, ?it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 75/75 [11:38<00:00,  9.31s/it]\n","                 all       1186          0          0          0          0          0\n","Speed: 0.2ms pre-process, 8.1ms inference, 1.8ms NMS per image at shape (16, 3, 640, 640)\n","Results saved to \u001b[1m/content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU100K-6799-Cropped_images_results/16_640_50/exp\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"g_RjKGmaLFaJ"},"source":["## 3. Detect\n","\n","* Use following command to detect the objects from images. You can set your detection images path using `--source` argument. \n","\n","* All detecting results are saved to runs/detect/ with incrementing run directories, i.e. runs/detect/exp2, runs/detect/exp3 etc., if `--project` argument is not given."]},{"cell_type":"code","metadata":{"id":"HVCp_nTSC13z"},"source":["# To detect objects from given source path's images\n","# To detect test_0.jpg\n","\n","!python detect.py --weights /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU110K-6799_16_640_50_crops_best.pt --img 640  --source /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU-110K/images/test_0.jpg --project /content/gdrive/MyDrive/Object_Detection_in_Dense_Conditions/SKU100K-6799-Cropped_images_results/16_640_50"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pFWrp7y3LerP"},"source":["# To Convert Folder into Zip Folder"]},{"cell_type":"code","metadata":{"id":"j0mW_pZDLdog"},"source":["import os\n","from os import path\n","import shutil\n","from shutil import make_archive\n","from zipfile import ZipFile\n","!zip -r /content/results.zip /content/yolov5/runs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OMFDiLgxLsJH"},"source":["# To Download Zipped Folder from Colab to Local PC"]},{"cell_type":"code","metadata":{"id":"O9aELHdYLsgc"},"source":["from google.colab import files\n","files.download('/content/results.zip')"],"execution_count":null,"outputs":[]}]}